{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --report              Print a detailed classification report\n",
      "  --chi2_select=SELECT_CHI2\n",
      "                        Select some number of features using a chi-squared\n",
      "                        test\n",
      "  --confusion_matrix    Print the confusion matrix\n",
      "  --top 10              Print ten most discriminative terms per classfor every\n",
      "                        classifier\n",
      "  --all_categories      Whether to use all categories or not\n",
      "  --use_hashing         use a hashing vectorizer\n",
      "  --n_features=N_FEATURES\n",
      "                        n_features when using the Hashing vectorizer\n",
      "  --filtered            remove information that is easily overfit:headers,\n",
      "                        signatures, and quoting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level = logging.INFO, format = '%(asctime)s %(Levelname)s %(message)s')\n",
    "\n",
    "op = OptionParser()\n",
    "op.add_option(\"--report\", action =\"store_true\", dest =\"print_report\", help=\"Print a detailed classification report\")\n",
    "op.add_option(\"--chi2_select\", action =\"store\", type=\"int\", dest =\"select_chi2\", help =\"Select some number of features using a chi-squared test\")\n",
    "op.add_option(\"--confusion_matrix\", action =\"store_true\", dest=\"print_cm\", help=\"Print the confusion matrix\")\n",
    "op.add_option(\"--top 10\", action =\"store_true\", dest=\"print_top10\", help=\"Print ten most discriminative terms per class\" \"for every classifier\")\n",
    "op.add_option(\"--all_categories\", action=\"store_true\", dest=\"all_categories\", help=\"Whether to use all categories or not\")\n",
    "op.add_option(\"--use_hashing\", action=\"store_true\", help=\"use a hashing vectorizer\")\n",
    "op.add_option(\"--n_features\", action=\"store\", type=int, default = 2 ** 16, help = \"n_features when using the Hashing vectorizer\")\n",
    "op.add_option(\"--filtered\", action=\"store_true\", help=\"remove information that is easily overfit:\" \"headers, signatures, and quoting\")\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'],'__file__')\n",
    "\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no argument.\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\logging\\__init__.py\", line 992, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\logging\\__init__.py\", line 838, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\logging\\__init__.py\", line 578, in format\n",
      "    s = self.formatMessage(record)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\logging\\__init__.py\", line 547, in formatMessage\n",
      "    return self._style.format(record)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\logging\\__init__.py\", line 391, in format\n",
      "    return self._fmt % record.__dict__\n",
      "KeyError: 'Levelname'\n",
      "Call stack:\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3242, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-249-0235478a3d96>\", line 14, in <module>\n",
      "    data_train = fetch_20newsgroups(subset ='train', categories = categories, shuffle = True, random_state =42, remove = remove)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py\", line 248, in fetch_20newsgroups\n",
      "    logger.info(\"Downloading 20news dataset. \"\n",
      "Message: 'Downloading 20news dataset. This may take a few minutes.'\n",
      "Arguments: ()\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroup datasets for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\logging\\__init__.py\", line 992, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\logging\\__init__.py\", line 838, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\logging\\__init__.py\", line 578, in format\n",
      "    s = self.formatMessage(record)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\logging\\__init__.py\", line 547, in formatMessage\n",
      "    return self._style.format(record)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\logging\\__init__.py\", line 391, in format\n",
      "    return self._fmt % record.__dict__\n",
      "KeyError: 'Levelname'\n",
      "Call stack:\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3242, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-249-0235478a3d96>\", line 14, in <module>\n",
      "    data_train = fetch_20newsgroups(subset ='train', categories = categories, shuffle = True, random_state =42, remove = remove)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py\", line 251, in fetch_20newsgroups\n",
      "    cache_path=cache_path)\n",
      "  File \"C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py\", line 72, in _download_20newsgroups\n",
      "    logger.info(\"Downloading dataset from %s (14 MB)\", ARCHIVE.url)\n",
      "Message: 'Downloading dataset from %s (14 MB)'\n",
      "Arguments: ('https://ndownloader.figshare.com/files/5975967',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "if opts.all_categories:\n",
    "    categories = None\n",
    "else:\n",
    "    categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "    \n",
    "if opts.filtered:\n",
    "    remove = ('headers', 'footers', 'quotes')\n",
    "else: \n",
    "    remove =()\n",
    "    \n",
    "print('Loading 20 newsgroup datasets for categories:')\n",
    "print(categories if categories else 'all')\n",
    "\n",
    "data_train = fetch_20newsgroups(subset ='train', categories = categories, shuffle = True, random_state =42, remove = remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"I am a boy\"\n",
    "print(1e6)\n",
    "len(string.encode('utf-8'))/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: rych@festival.ed.ac.uk (R Hawkes)\\nSubject: 3DS: Where did all the texture rules go?\\nLines: 21\\n\\nHi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\\n\\n======================================================================\\nRycharde Hawkes\\t\\t\\t\\temail: rych@festival.ed.ac.uk\\nVirtual Environment Laboratory\\nDept. of Psychology\\t\\t\\tTel  : +44 31 650 3426\\nUniv. of Edinburgh\\t\\t\\tFax  : +44 31 667 0150\\n======================================================================\\n\""
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1022"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train.data[0].encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.979536"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 documents - 3.980MB (training set)\n",
      "1353 documents - 2.867MB (test set)\n",
      "4 categories\n",
      "\n",
      "Extracting features from the training data using sparse vectorizer\n",
      "done in 0.456778s at 8.712MB/s\n",
      "n_samples: 2034, n_features: 33809\n",
      "\n",
      "Extracting features from the test data using the same vectorizer\n",
      "done in 0.313166s at 9.156MB/s\n",
      "n_samples: 1353, n_features: 33809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = data_train.target_names\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs)/1e6\n",
    "\n",
    "data_train_size_mb = size_mb(data_train.data)\n",
    "data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" %(len(data_train.data), data_train_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" %(len(data_test.data), data_test_size_mb))\n",
    "print(\"%d categories\"% len(target_names))\n",
    "print()\n",
    "\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features from the training data using sparse vectorizer\")\n",
    "\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    vectorizer = HashingVectorizer(stop_words='english', alternate_sign = False, n_features=opts.n_features)\n",
    "    X_train = vectorizer.transform(data_train.data)\n",
    "else:\n",
    "    vectorizer=TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "    \n",
    "duration = time() - t0\n",
    "print('done in %fs at %0.3fMB/s' %(duration, data_train_size_mb/duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb/duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n",
    "if opts.use_hashing:\n",
    "    feature_names = None\n",
    "else:\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "if opts.select_chi2:\n",
    "    print(\"Extracting %d best features by a chi-squared test\" % opts.select_chi2)\n",
    "    t0 = time()\n",
    "    ch2 = SelectKBest(chi2, k = opts.select_chi2)\n",
    "    X_train = ch2.fit_transform(X_train, y_train)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    if feature_names:\n",
    "        feature_names = [feature_names[i] for i in ch2.get_support(indices = True)]\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    print()\n",
    "    \n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "    \n",
    "def trim(s):\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "                max_iter=None, normalize=False, random_state=None, solver='sag',\n",
      "                tol=0.01)\n",
      "train time: 0.174s\n",
      "test time: 0.007s\n",
      "accuracy: 0.897\n",
      "dimensionality: 33809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zenit\\.julia\\conda\\3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:558: UserWarning: \"sag\" solver requires many iterations to fit an intercept with sparse inputs. Either set the solver to \"auto\" or \"sparse_cg\", or set a low \"tol\" and a high \"max_iter\" (especially if inputs are not standardized).\n",
      "  '\"sag\" solver requires many iterations to fit '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density: 1.000000\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
      "           fit_intercept=True, max_iter=50, n_iter_no_change=5, n_jobs=None,\n",
      "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
      "           validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "train time: 0.021s\n",
      "test time: 0.002s\n",
      "accuracy: 0.888\n",
      "dimensionality: 33809\n",
      "density: 0.255302\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "                            early_stopping=False, fit_intercept=True,\n",
      "                            loss='hinge', max_iter=50, n_iter_no_change=5,\n",
      "                            n_jobs=None, random_state=None, shuffle=True,\n",
      "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
      "                            warm_start=False)\n",
      "train time: 0.036s\n",
      "test time: 0.004s\n",
      "accuracy: 0.902\n",
      "dimensionality: 33809\n",
      "density: 0.712088\n",
      "\n",
      "\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "train time: 0.003s\n",
      "test time: 0.159s\n",
      "accuracy: 0.858\n",
      "\n",
      "================================================================================\n",
      "Random Forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "train time: 1.205s\n",
      "test time: 0.062s\n",
      "accuracy: 0.828\n",
      "\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
      "              n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "train time: 0.138s\n",
      "test time: 0.002s\n",
      "accuracy: 0.898\n",
      "dimensionality: 33809\n",
      "density: 0.184618\n",
      "\n",
      "\n",
      "================================================================================\n",
      "NearestCentroid(aka Rocchio classifier)\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "train time: 0.004s\n",
      "test time: 0.004s\n",
      "accuracy: 0.855\n",
      "\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.010s\n",
      "test time: 0.004s\n",
      "accuracy: 0.899\n",
      "dimensionality: 33809\n",
      "density: 1.000000\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.011s\n",
      "test time: 0.009s\n",
      "accuracy: 0.884\n",
      "dimensionality: 33809\n",
      "density: 1.000000\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "ComplementNB(alpha=0.1, class_prior=None, fit_prior=True, norm=False)\n",
      "train time: 0.009s\n",
      "test time: 0.002s\n",
      "accuracy: 0.911\n",
      "dimensionality: 33809\n",
      "density: 1.000000\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Linear SVC with L1 based feature selection\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Pipeline(memory=None,\n",
      "         steps=[('feature_selection',\n",
      "                 SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None,\n",
      "                                                     dual=False,\n",
      "                                                     fit_intercept=True,\n",
      "                                                     intercept_scaling=1,\n",
      "                                                     loss='squared_hinge',\n",
      "                                                     max_iter=1000,\n",
      "                                                     multi_class='ovr',\n",
      "                                                     penalty='l1',\n",
      "                                                     random_state=None,\n",
      "                                                     tol=0.001, verbose=0),\n",
      "                                 max_features=None, norm_order=1, prefit=False,\n",
      "                                 threshold=None)),\n",
      "                ('classification',\n",
      "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
      "                           fit_intercept=True, intercept_scaling=1,\n",
      "                           loss='squared_hinge', max_iter=1000,\n",
      "                           multi_class='ovr', penalty='l2', random_state=None,\n",
      "                           tol=0.0001, verbose=0))],\n",
      "         verbose=False)\n",
      "train time: 0.283s\n",
      "test time: 0.003s\n",
      "accuracy: 0.880\n",
      "\n",
      "================================================================================\n",
      "12 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='12', random_state=None, tol=0.001,\n",
      "          verbose=0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported set of arguments: The combination of penalty='12' and loss='squared_hinge' is not supported, Parameters: penalty='12', loss='squared_hinge', dual=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-258-be3a1762099d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s penalty\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# Train Liblinear model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# Train SGD model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-258-be3a1762099d>\u001b[0m in \u001b[0;36mbenchmark\u001b[1;34m(clf)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train time: %0.3fs\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\lib\\site-packages\\sklearn\\svm\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"crammer_singer\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    933\u001b[0m                                          dtype=np.float64)\n\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m     \u001b[0msolver_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[0;32m    937\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_get_liblinear_solver_type\u001b[1;34m(multi_class, penalty, loss, dual)\u001b[0m\n\u001b[0;32m    791\u001b[0m     raise ValueError('Unsupported set of arguments: %s, '\n\u001b[0;32m    792\u001b[0m                      \u001b[1;34m'Parameters: penalty=%r, loss=%r, dual=%r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m                      % (error_string, penalty, loss, dual))\n\u001b[0m\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported set of arguments: The combination of penalty='12' and loss='squared_hinge' is not supported, Parameters: penalty='12', loss='squared_hinge', dual=False"
     ]
    }
   ],
   "source": [
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "    \n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time: %0.3fs\" % test_time)\n",
    "    \n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy: %0.3f\" % score)\n",
    "    \n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "        \n",
    "        if opts.print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class: \")\n",
    "            for i, label in enumerate(target_names):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "        print()\n",
    "    \n",
    "    if opts.print_report:\n",
    "        print(\"Classification report\")\n",
    "        print(metrics.classification_report(y_test, pred, target_names=target_names))\n",
    "        \n",
    "    if opts.print_cm:\n",
    "        print(\"confusion_matrix: \")\n",
    "        print(metrics.confusion_matrix(y_test, pred))\n",
    "        \n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "results = []\n",
    "for clf, name in ((RidgeClassifier(tol =1e-2, solver=\"sag\"), \"Ridge Classifier\"),\n",
    "                 (Perceptron(max_iter=50), \"Perceptron\"),\n",
    "                 (PassiveAggressiveClassifier(max_iter=50), \"Passive-Aggressive\"), \n",
    "                 (KNeighborsClassifier(n_neighbors=10), 'kNN'), \n",
    "                 (RandomForestClassifier(), \"Random Forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print('Elastic-Net penalty')\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50, penalty='elasticnet')))\n",
    "\n",
    "# Train nearest centroid without threshold\n",
    "print('=' * 80)\n",
    "print('NearestCentroid(aka Rocchio classifier)')\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "#Train sparse Naive Bayes classifier\n",
    "print('=' * 80)\n",
    "print('Naive Bayes')\n",
    "\n",
    "results.append(benchmark(MultinomialNB(alpha=0.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=0.01)))\n",
    "results.append(benchmark(ComplementNB(alpha = 0.1)))\n",
    "    \n",
    "    \n",
    "print('=' * 80)\n",
    "print(\"Linear SVC with L1 based feature selection\")\n",
    "\n",
    "results.append(benchmark(Pipeline([('feature_selection', SelectFromModel(LinearSVC(penalty='l1', dual = False, tol=1e-3))), \n",
    "                                  ('classification', LinearSVC(penalty='l2'))])))\n",
    "    \n",
    "for penalty in ['12', '11']:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(penalty=penalty, dual = False, tol = 1e-3)))\n",
    "    \n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50, penalty = penalty)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/zenit/Desktop/Sahal backup/Northeastern University/Datasets/AV identify sentiments/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does my data look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
       "1   2      0  Finally a transparant silicon case ^^ Thanks t...\n",
       "2   3      0  We love this! Would you go? #talk #makememorie...\n",
       "3   4      0  I'm wired I know I'm George I was made that wa...\n",
       "4   5      1  What amazing service! Apple won't even talk to..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See a negative tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tweet[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have 5894 positive tweets and 2026 negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    74.419192\n",
       "1    25.580808\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()*100/len(train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x250f23affd0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAARoElEQVR4nO3df6zd9V3H8edrdBOdbhQpiC2s6JopU7exG0CXGDe0FPxRVDAsTm6wSU3EH0uMyvzDKohx8cfc5kbSSLcyp0jQSWOWYdNtGn+wceuQDTrSipPeFGm3duBcNmW+/eN+rjtt772fQ7nn3Fvu85GcnO/n/f18v+d9lobXvj/O96aqkCRpIS9Y6gYkScufYSFJ6jIsJEldhoUkqcuwkCR1rVrqBkbhnHPOqfXr1y91G5J0Wtm7d+9nq2rNXOuel2Gxfv16pqamlroNSTqtJPn3+dZ5GkqS1GVYSJK6RhoWSc5Kck+STyfZl+S7k5ydZHeS/e19dZubJO9IciDJQ0kuGdjPZJu/P8nkKHuWJJ1s1EcWbwc+VFXfBrwK2AfcDOypqg3AnjYGuArY0F5bgdsBkpwNbAMuAy4Fts0GjCRpPEYWFkleAnwvcAdAVf13VX0e2AzsbNN2Ate05c3AnTXjfuCsJOcDVwK7q+poVR0DdgObRtW3JOlkozyy+BbgCPCeJJ9I8sdJXgycV1VPALT3c9v8tcDBge2nW22++nGSbE0ylWTqyJEji/9tJGkFG2VYrAIuAW6vqtcA/8VXTznNJXPUaoH68YWq7VU1UVUTa9bMeZuwJOkUjTIspoHpqvpYG9/DTHg82U4v0d4PD8y/YGD7dcChBeqSpDEZWVhU1X8AB5O8opWuAB4BdgGzdzRNAve25V3ADe2uqMuBp9ppqvuAjUlWtwvbG1tNkjQmo/4F988D70/yIuAx4EZmAuruJFuAx4Hr2twPAlcDB4AvtrlU1dEktwIPtHm3VNXREffNa3/5zlF/hE5De3/3hqVuQVoSIw2LqnoQmJhj1RVzzC3gpnn2swPYsbjdSZKG5S+4JUldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqWukYZHkM0k+meTBJFOtdnaS3Un2t/fVrZ4k70hyIMlDSS4Z2M9km78/yeQoe5YknWwcRxavr6pXV9VEG98M7KmqDcCeNga4CtjQXluB22EmXIBtwGXApcC22YCRJI3HUpyG2gzsbMs7gWsG6nfWjPuBs5KcD1wJ7K6qo1V1DNgNbBp305K0ko06LAr4myR7k2xttfOq6gmA9n5uq68FDg5sO91q89WPk2RrkqkkU0eOHFnkryFJK9uqEe//dVV1KMm5wO4kn15gbuao1QL14wtV24HtABMTEyetlySdupEeWVTVofZ+GPgAM9ccnmynl2jvh9v0aeCCgc3XAYcWqEuSxmRkYZHkxUm+YXYZ2Ah8CtgFzN7RNAnc25Z3ATe0u6IuB55qp6nuAzYmWd0ubG9sNUnSmIzyNNR5wAeSzH7On1bVh5I8ANydZAvwOHBdm/9B4GrgAPBF4EaAqjqa5FbggTbvlqo6OsK+JUknGFlYVNVjwKvmqH8OuGKOegE3zbOvHcCOxe5RkjQcf8EtSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXSMPiyRnJPlEkr9u44uSfCzJ/iR/nuRFrf41bXygrV8/sI+3tPqjSa4cdc+SpOON48jiF4F9A+O3Am+rqg3AMWBLq28BjlXVy4G3tXkkuRi4HnglsAl4d5IzxtC3JKkZaVgkWQf8IPDHbRzgDcA9bcpO4Jq2vLmNaeuvaPM3A3dV1Zer6t+AA8Clo+xbknS8UR9Z/CHwK8D/tvE3Ap+vqmfaeBpY25bXAgcB2vqn2vz/r8+xzf9LsjXJVJKpI0eOLPb3kKQVbWRhkeSHgMNVtXewPMfU6qxbaJuvFqq2V9VEVU2sWbPmWfcrSZrfqhHu+3XAjyS5GjgTeAkzRxpnJVnVjh7WAYfa/GngAmA6ySrgpcDRgfqswW0kSWMwsiOLqnpLVa2rqvXMXKD+cFX9JPAR4No2bRK4ty3vamPa+g9XVbX69e1uqYuADcDHR9W3JOlkozyymM+vAncl+S3gE8AdrX4H8L4kB5g5orgeoKoeTnI38AjwDHBTVX1l/G1L0so1lrCoqo8CH23LjzHH3UxV9SXgunm2vw24bXQdSpIW4i+4JUldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUtdQYZFkzzA1SdLz04J//CjJmcDXAeckWQ2krXoJ8M0j7k2StEz0/lLezwBvZiYY9vLVsHgaeNcI+5IkLSMLhkVVvR14e5Kfr6p3jqknSdIyM9Tf4K6qdyb5HmD94DZVdeeI+pIkLSNDhUWS9wHfCjwIfKWVCzAsJGkFGCosgAng4qqqUTYjSVqehv2dxaeAbxplI5Kk5WvYI4tzgEeSfBz48myxqn5kJF1JkpaVYcPiN0bZhCRpeRv2bqi/HXUjkqTla9jHffxnkqfb60tJvpLk6c42Zyb5eJJ/SfJwkt9s9YuSfCzJ/iR/nuRFrf41bXygrV8/sK+3tPqjSa489a8rSToVQ4VFVX1DVb2kvc4Efhz4o85mXwbeUFWvAl4NbEpyOfBW4G1VtQE4Bmxp87cAx6rq5cDb2jySXAxcD7wS2AS8O8kZz+ZLSpKem1N66mxV/RXwhs6cqqovtOEL26vadve0+k7gmra8uY1p669Ikla/q6q+XFX/BhwALj2VviVJp2bYH+X92MDwBcz87qL7m4t2BLAXeDkzz5L6V+DzVfVMmzINrG3La4GDAFX1TJKngG9s9fsHdju4zeBnbQW2Alx44YXDfC1J0pCGvRvqhweWnwE+w8z/419QVX0FeHWSs4APAN8+17T2nnnWzVc/8bO2A9sBJiYm/PGgJC2iYe+GuvG5fEhVfT7JR4HLgbOSrGpHF+uAQ23aNHABMJ1kFfBS4OhAfdbgNpKkMRj2bqh1ST6Q5HCSJ5P8RZJ1nW3WtCMKknwt8P3APuAjwLVt2iRwb1ve1ca09R9ujxfZBVzf7pa6CNgAfHz4ryhJeq6GPQ31HuBPgeva+E2t9gMLbHM+sLNdt3gBcHdV/XWSR4C7kvwW8Angjjb/DuB9SQ4wc0RxPUBVPZzkbuARZk6B3dROb0mSxmTYsFhTVe8ZGL83yZsX2qCqHgJeM0f9Mea4m6mqvsRXw+jEdbcBtw3ZqyRpkQ176+xnk7wpyRnt9Sbgc6NsTJK0fAwbFj8N/ATwH8ATzFxTeE4XvSVJp49hT0PdCkxW1TGAJGcDv8dMiEiSnueGPbL4rtmgAKiqo8xxPUKS9Pw0bFi8IMnq2UE7shj2qESSdJob9j/4vw/8Y5J7mPn19E/g3UmStGIM+wvuO5NMMfMQwAA/VlWPjLQzSdKyMfSppBYOBoQkrUCn9IhySdLKYlhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSukYWFkkuSPKRJPuSPJzkF1v97CS7k+xv76tbPUnekeRAkoeSXDKwr8k2f3+SyVH1LEma2yiPLJ4Bfqmqvh24HLgpycXAzcCeqtoA7GljgKuADe21FbgdZsIF2AZcBlwKbJsNGEnSeIwsLKrqiar657b8n8A+YC2wGdjZpu0ErmnLm4E7a8b9wFlJzgeuBHZX1dGqOgbsBjaNqm9J0slWjeNDkqwHXgN8DDivqp6AmUBJcm6bthY4OLDZdKvNV5dWpMdv+c6lbkHL0IW//smR7n/kF7iTfD3wF8Cbq+rphabOUasF6id+ztYkU0mmjhw5cmrNSpLmNNKwSPJCZoLi/VX1l638ZDu9RHs/3OrTwAUDm68DDi1QP05Vba+qiaqaWLNmzeJ+EUla4UZ5N1SAO4B9VfUHA6t2AbN3NE0C9w7Ub2h3RV0OPNVOV90HbEyyul3Y3thqkqQxGeU1i9cBPwV8MsmDrfZrwO8AdyfZAjwOXNfWfRC4GjgAfBG4EaCqjia5FXigzbulqo6OsG9J0glGFhZV9ffMfb0B4Io55hdw0zz72gHsWLzuJEnPhr/gliR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUNbKwSLIjyeEknxqonZ1kd5L97X11qyfJO5IcSPJQkksGtpls8/cnmRxVv5Kk+Y3yyOK9wKYTajcDe6pqA7CnjQGuAja011bgdpgJF2AbcBlwKbBtNmAkSeMzsrCoqr8Djp5Q3gzsbMs7gWsG6nfWjPuBs5KcD1wJ7K6qo1V1DNjNyQEkSRqxcV+zOK+qngBo7+e2+lrg4MC86Vabr36SJFuTTCWZOnLkyKI3Lkkr2XK5wJ05arVA/eRi1faqmqiqiTVr1ixqc5K00o07LJ5sp5do74dbfRq4YGDeOuDQAnVJ0hiNOyx2AbN3NE0C9w7Ub2h3RV0OPNVOU90HbEyyul3Y3thqkqQxWjWqHSf5M+D7gHOSTDNzV9PvAHcn2QI8DlzXpn8QuBo4AHwRuBGgqo4muRV4oM27papOvGguSRqxkYVFVb1xnlVXzDG3gJvm2c8OYMcitiZJepaWywVuSdIyZlhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkrpOm7BIsinJo0kOJLl5qfuRpJXktAiLJGcA7wKuAi4G3pjk4qXtSpJWjtMiLIBLgQNV9VhV/TdwF7B5iXuSpBVj1VI3MKS1wMGB8TRw2eCEJFuBrW34hSSPjqm3leAc4LNL3cRykN+bXOoWdDz/bc7alsXYy8vmW3G6hMVc/yvUcYOq7cD28bSzsiSZqqqJpe5DOpH/NsfndDkNNQ1cMDBeBxxaol4kacU5XcLiAWBDkouSvAi4Hti1xD1J0opxWpyGqqpnkvwccB9wBrCjqh5e4rZWEk/vabny3+aYpKr6syRJK9rpchpKkrSEDAtJUpdhoQX5mBUtR0l2JDmc5FNL3ctKYVhoXj5mRcvYe4FNS93ESmJYaCE+ZkXLUlX9HXB0qftYSQwLLWSux6ysXaJeJC0hw0IL6T5mRdLKYFhoIT5mRRJgWGhhPmZFEmBYaAFV9Qww+5iVfcDdPmZFy0GSPwP+CXhFkukkW5a6p+c7H/chSeryyEKS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhbQIknyhs379s31CapL3Jrn2uXUmLQ7DQpLUZVhIiyjJ1yfZk+Sfk3wyyeBTelcl2ZnkoST3JPm6ts1rk/xtkr1J7kty/hK1L83LsJAW15eAH62qS4DXA7+fZPaBjK8AtlfVdwFPAz+b5IXAO4Frq+q1wA7gtiXoW1rQqqVuQHqeCfDbSb4X+F9mHul+Xlt3sKr+oS3/CfALwIeA7wB2t0w5A3hirB1LQzAspMX1k8Aa4LVV9T9JPgOc2dad+GydYiZcHq6q7x5fi9Kz52koaXG9FDjcguL1wMsG1l2YZDYU3gj8PfAosGa2nuSFSV451o6lIRgW0uJ6PzCRZIqZo4xPD6zbB0wmeQg4G7i9/bnaa4G3JvkX4EHge8bcs9TlU2clSV0eWUiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpK7/A2allVRNCFR+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New iPhone OS on the 3G FUCKING SUCKS. I WANT TO THROW MY PHONE AGAINST THE WALL. HOW DOES THE SMS \"APP\" crash!?! #apple #iphone\n"
     ]
    }
   ],
   "source": [
    "index = 176\n",
    "example = train[train.index == index][['tweet', 'label']].values[0]\n",
    "if len(example) > 0:\n",
    "    print(example[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert all text to lowercase\n",
    "### Replace replace_by_space_re symbols by space in text\n",
    "### Remove symbols that are in Bad_Symbol_RE from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train.label\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train['tweet'], labels, test_size = 0.15, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6732,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1188,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert abstracts into word count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(strip_accents = 'ascii', lowercase = True, stop_words = 'english', token_pattern = u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the train and validation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv = cv.fit_transform(x_train)\n",
    "X_test_cv = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data frame to view your vectorized data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.DataFrame(X_train_cv.toarray(), columns = cv.get_feature_names())\n",
    "\n",
    "top_words_df = pd.DataFrame(word_freq_df.sum()).sort_values(0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01108nartl</th>\n",
       "      <th>01am</th>\n",
       "      <th>01amapril</th>\n",
       "      <th>01amjune</th>\n",
       "      <th>01ammay</th>\n",
       "      <th>01pm</th>\n",
       "      <th>01pmapril</th>\n",
       "      <th>01pmjune</th>\n",
       "      <th>...</th>\n",
       "      <th>zyuirs1wadbou</th>\n",
       "      <th>zyvwiyhtzm</th>\n",
       "      <th>zz7xpxpqmwkt</th>\n",
       "      <th>zz86j</th>\n",
       "      <th>zz96a_jep6</th>\n",
       "      <th>zzhb7x12hn6wg</th>\n",
       "      <th>zzita3co0x</th>\n",
       "      <th>zzjvgtyaxl</th>\n",
       "      <th>zznj908fjw</th>\n",
       "      <th>zznnb2inkt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6727</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6728</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6729</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6730</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6731</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6732 rows × 19938 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00am  00pm  01108nartl  01am  01amapril  01amjune  01ammay  01pm  \\\n",
       "0        0     0           0     0          0         0        0     0   \n",
       "1        0     0           0     0          0         0        0     0   \n",
       "2        0     0           0     0          0         0        0     0   \n",
       "3        0     0           0     0          0         0        0     0   \n",
       "4        0     0           0     0          0         0        0     0   \n",
       "...    ...   ...         ...   ...        ...       ...      ...   ...   \n",
       "6727     0     0           0     0          0         0        0     0   \n",
       "6728     0     0           0     0          0         0        0     0   \n",
       "6729     0     0           0     0          0         0        0     0   \n",
       "6730     0     0           0     0          0         0        0     0   \n",
       "6731     0     0           0     0          0         0        0     0   \n",
       "\n",
       "      01pmapril  01pmjune  ...  zyuirs1wadbou  zyvwiyhtzm  zz7xpxpqmwkt  \\\n",
       "0             0         0  ...              0           0             0   \n",
       "1             0         0  ...              0           0             0   \n",
       "2             0         0  ...              0           0             0   \n",
       "3             0         0  ...              0           0             0   \n",
       "4             0         0  ...              0           0             0   \n",
       "...         ...       ...  ...            ...         ...           ...   \n",
       "6727          0         0  ...              0           0             0   \n",
       "6728          0         0  ...              0           0             0   \n",
       "6729          0         0  ...              0           0             0   \n",
       "6730          0         0  ...              0           0             0   \n",
       "6731          0         0  ...              0           0             0   \n",
       "\n",
       "      zz86j  zz96a_jep6  zzhb7x12hn6wg  zzita3co0x  zzjvgtyaxl  zznj908fjw  \\\n",
       "0         0           0              0           0           0           0   \n",
       "1         0           0              0           0           0           0   \n",
       "2         0           0              0           0           0           0   \n",
       "3         0           0              0           0           0           0   \n",
       "4         0           0              0           0           0           0   \n",
       "...     ...         ...            ...         ...         ...         ...   \n",
       "6727      0           0              0           0           0           0   \n",
       "6728      0           0              0           0           0           0   \n",
       "6729      0           0              0           0           0           0   \n",
       "6730      0           0              0           0           0           0   \n",
       "6731      0           0              0           0           0           0   \n",
       "\n",
       "      zznnb2inkt  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "6727           0  \n",
       "6728           0  \n",
       "6729           0  \n",
       "6730           0  \n",
       "6731           0  \n",
       "\n",
       "[6732 rows x 19938 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Multinomial NB\n",
    "#naive_bayes = MultinomialNB(alpha = 0.1)\n",
    "\n",
    "# Bernoulli NB\n",
    "naive_bayes = BernoulliNB(alpha = 0.63)\n",
    "\n",
    "# Complement NB\n",
    "#naive_bayes = ComplementNB(alpha = 0.085)\n",
    "\n",
    "naive_bayes.fit(X_train_cv, y_train)\n",
    "predictions = naive_bayes.predict(X_test_cv)\n",
    "\n",
    "#Gaussian NB here\n",
    "# gnb = GaussianNB(alpha = 0.1)\n",
    "# y_pred = gnb.fit(X_train_cv.toarray(), y_train).predict(X_test_cv.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8930976430976431"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "accuracy_score(y_test, predictions)\n",
    "\n",
    "#accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "#precision_score(y_test, predictions)\n",
    "#recall_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1 = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.30474126e-04, 9.99769526e-01])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.predict_proba(X_test_cv)[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(51.0, 0.5, 'True label')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAEbCAYAAAD6Yay4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAbE0lEQVR4nO3deZgddZ3v8fc3HUgCSdiDQNiG5QCKbBEE4wwQNlEERBBRrgskAlcBUebKNiLKBS+iXnFhlU1HFgFBtgRxAEEEhiVsSYNsIvsWAwmELN/5o6pDp+1uDqSru3J4v57nPKdOVXV9fwdS/enafr/ITCRJUn0NGugGSJKk3hnWkiTVnGEtSVLNGdaSJNWcYS1JUs0Z1pIk1ZxhLYAGcE+n13Tg0C7rHN5p+f3AXGDZhaw7BLgQ+CtwG7BGOX974E7gvvJ924WsI7WqrwMPUOyTvwGGAuOAuyj21ZuBtQesdeoz4XPW6qINeArYAniih3V2ofgl0WyIrgGcA2zdZf5BwAeBA4C9gd2BzwCbAM8BTwMfACYCqzRZS3qvWIUijDcAXgcuAq4GjgR2BaZQ7GObA18cmCaqr3hkra7GAY/Qc1ADfJbir/gOnwdup/hL/jSKwG/GrsC55fRvy9oB3E0R1FAcNQylOAqXtKDBwLDyfQmK/SaBkeXypXhrX9IirPKwjojVI2K7cnpYRIyouqYWyt4sGMRdLQHsBFxSfl6f4mj4I8DGFKfHP9dkrVWAJ8vpOcA/gOW6rLMHRXjPanKb0nvFU8APgL8Bz1DsP5OA/SmOsP8O7AucOFANVN+pNKwjYjzFEdNp5azRwO+qrKmFsjjwSeDiXtbZBbgFeLn8PA7YDLiD4sh6HPAv5bLLynlXA2N465r3l8rl0c32O1+XeT/wfeAr7/B7SO8Fy1CcnVoTWBlYkuIs19eBnSl+354N/HCgGqi+U+k164i4h+J6yW2ZuUk5777M3LCH9ScAEwDaRm+12aDlG5W1Tf9sl6035oC9tuXjB/W8b1988le55Lo7uODa2wA4aO9xrLzC0hx9yiU9/szqKy3Hmcftx/bj/98C86/82WF897TLue3eR2hrG8ST1/2Ilbc9BIBVRi3DpNMPZ/9v/5JbJ/+1D76dmvXTx/880E1QE9bcdUdWHfdRbjr4aADW+cyurLj5xoze5iNcsOkOACw5eiV2vvgMLt7yEwPZVL0DE16Z2t1BTOWnwWdl5psdHyJiMAseOS0gM0/PzDGZOcag7n+f2WkLLrz29h6Xjxw+jI9uti5X3HD3/Hn/dfsUdt9uDCssU1zdWGbkkqy2Utcz2d278sZ72HeXrQDYY7sx3HDHVACWGj6My085lKNPucSglnrw2t+fYdSYjWgbNhSAVf5tS16Z+giLjxzBUmutAcDorbdi2kOPDmAr1VcGV7z9GyPiSGBYRGxPcWfi7yuuqXdh2NDFGbfF+znoe+fNnzf+01sDcMZvbwBg12025Q9/eYCZb8z/+4spjz7NsT+7lKt/8Q0GRTB7zlwOPvFX/O2Zl9625tm/u4lzvjeeBy8/gVemz+Dz3yqulhy09zjWWnUUR47fhSPH7wLAzgeezAuvvNpH31Za9L1w5708dsUk9rjhUubNncNL905hyrkXMuPpZ9n+vJ+Q8+Yxa9p0bvzqkQPdVPWBqk+DDwL2A3aguD45ETgzmyi6+CZf9pkyaQB4GlwaOD2dBq/6yHpX4LzMPKPiOpIktayqr1l/EngoIs6PiI+X16wlSdI7UGlYZ+aXKLq6uxjYB3gkIs6ssqYkSa2m8iPdzJwdEddQ3AU+jOLU+P5V15UkqVVU3SnKThFxDsVADZ8GzgRWqrKmJEmtpuoj6y8CFwBfyUy7i5Qk6V2oNKwzc+8qty9J0ntBJWEdETdn5tiIeJUFeywLIDNzZA8/KkmSuqgkrDNzbPnuCFuSJC2kqm8wO7+ZeZIkqWdVd4ry/s4fyk5RNqu4piRJLaWSsI6II8rr1R+MiOnl61XgOeDyKmpKktSqKgnrzDyhvF59UmaOLF8jMnO5zDyiipqSJLWqqu4GXy8zpwIXR8SmXZdn5l1V1JUkqRVV9Zz1YcAE4ORuliWwbUV1JUlqOVU9ujWhfN+miu1LkvReUvWjW3tGxIhy+uiIuDQiNqmypiRJrabqR7eOycxXI2IssCNwLnBqxTUlSWopVYf13PL948AvMvNyYPGKa0qS1FKqDuunIuI0YC/g6ogY0g81JUlqKVUH517ARGCnzJwGLAscXnFNSZJaSqVhnZkzgUeAHSPiq8CozJxUZU1JklpN1XeDHwL8GhhVvn4VEV+rsqYkSa2mqk5ROuwHbJGZMwAi4vvArcApFdeVJKllVH3NOnjrjnDK6ai4piRJLaXqI+uzgdsi4rLy827AWRXXlCSppVQa1pn5w4i4ARhLcUT9pcy8u8qakiS1mqpG3RoKHACsDdwH/Dwz51RRS5KkVlfVNetzgTEUQf0x4AcV1ZEkqeVVdRp8g8zcECAizgJur6iOJEktr6oj69kdE57+liRp4VR1ZL1RREwvpwMYVn4OIDNzZEV1JUlqOZWEdWa2VbFdSZLeixwBS5KkmjOsJUmqOcNakqSaM6wlSao5w1qSpJozrCVJqjnDWpKkmjOsJUmqOcNakqSaM6wlSao5w1qSpJozrCVJqjnDWpKkmjOsJUmqOcNakqSaM6wlSao5w1qSpJozrCVJqjnDWpKkmjOsJUmqOcNakqSaM6wlSao5w1qSpJozrCVJqjnDWpKkmjOsJUmqOcNakqSaM6wlSao5w1qSpJozrCVJqjnDWpKkmjOsJUmqOcNakqSaM6wlSao5w1qSpJozrCVJqjnDWpKkmjOsJUmqOcNakqSaM6wlSao5w1qSpJozrCVJqjnDWpKkmjOsJUmqOcNakqSaM6wlSao5w1qSpJozrCVJqjnDWpKkmjOsJUmqOcNakqSaG9zTgogY2dsPZub0vm+OJEnqqsewBh4AEohO8zo+J7Bahe2SJEmlHsM6M1ftz4ZIkqTuNXXNOiL2jogjy+nREbFZtc2SJEkd3jasI+KnwDbAvuWsmcCpVTZKkiS9pbdr1h22ysxNI+JugMx8OSIWr7hdkiSp1Mxp8NkRMYjipjIiYjlgXqWtkiRJ8zUT1j8DLgFWiIjvADcD36+0VZIkab63PQ2emedFxJ3AduWsPTPz/mqbJUmSOjRzzRqgDZhNcSrcXs8kSepHzdwNfhTwG2BlYDTwnxFxRNUNkyRJhWaOrD8PbJaZMwEi4njgTuCEKhsmSZIKzZzSfoIFQ30w8Gg1zZEkSV31NpDHjyiuUc8EHoiIieXnHSjuCJckSf2gt9PgHXd8PwBc1Wn+X6prjiRJ6qq3gTzO6s+GSJKk7r3tDWYRsRZwPLABMLRjfmauW2G7JElSqZkbzM4BzqYYx/pjwEXABRW2SZIkddJMWC+RmRMBMvORzDyaYhQuSZLUD5p5znpWRATwSEQcADwFjKq2WZIkqUMzYf11YDhwMMW166WAL1fZKEmS9JZmBvK4rZx8Fdi32uZIkqSueusU5TLKMay7k5mfqqRFkiRpAb0dWf+031rRjS/f88eBLC+9Z33h+uMGugmSuuitU5Tr+7MhkiSpe45NLUlSzRnWkiTVXNNhHRFDqmyIJEnq3tuGdURsHhH3AQ+XnzeKiFMqb5kkSQKaO7L+CfAJ4CWAzJyM3Y1KktRvmgnrQZn5RJd5c6tojCRJ+mfNdDf6ZERsDmREtAFfAx6qtlmSJKlDM0fWBwKHAasBzwEfLudJkqR+0Ezf4M8De/dDWyRJUjfeNqwj4gy66SM8MydU0iJJkrSAZq5Z/6HT9FBgd+DJapojSZK6auY0+IWdP0fE+cB1lbVIkiQt4N10N7omsHpfN0SSJHWvmWvWr/DWNetBwMvAt6pslCRJekuvYR0RAWwEPFXOmpeZ/3SzmSRJqk6vp8HLYL4sM+eWL4NakqR+1sw169sjYtPKWyJJkrrV42nwiBicmXOAscD4iHgEmAEExUG3AS5JUj/o7Zr17cCmwG791BZJktSN3sI6ADLzkX5qiyRJ6kZvYb1CRBzW08LM/GEF7ZEkSV30FtZtwHDKI2xJkjQwegvrZzLzuH5riSRJ6lZvj255RC1JUg30Ftbj+q0VkiSpRz2GdWa+3J8NkSRJ3Xs3o25JkqR+ZFhLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1Z1hLklRzhrUkSTVnWEuSVHOGtSRJNWdYS5JUc4a1JEk1N3igG6BF2+AhQ/jmTRcyeMgQBg1u467fXsOVx/6Ib9x0EUNHDAdgxKjlePz2yZy6+4QBbq1UL8++PJ2jzr2CF6e/xqAI9hi7CZ/fdvMF1pk+43X+4/wrefLFaQwZ3MZ39v0E66wyaqHqvjl7DkedewUP/u1ZllpyGCftvzurLLc0t055lB9f9l/MnjuXxdraOOxT49hivTUWqpb6hmGthTJn1ix+tO0+zJoxk0GDB3P4zb/lgWtu4OR/3Wv+OhN++wsmX37dALZSqqe2tuAbe4xjg9VWYsYbs9j7hF+y5fprstZKK8xf54xr/0xj9Ir8+IA9eezZFzn+gomceejnmtr+Uy9N45hzf88vD9t3gfmX/vkeRi4xlKuOO4hr7niAH1/2R07a/1MsPXwJTjloL0YtPYKHn3qeA0/5DX848ZA+/c56dzwNroU2a8ZMANoWG0zbYoPJzPnLhgxfksa2WzH5d5MGqnlSba2w1Ag2WG0lAJYcOoQ137ccz097dYF1Hn32BbZYb00A1nzf8jz90jRemv4aAFfedh/7nPhL9jz+DI779dXMnTevqbo3TH6YT374gwBsv+n63Db1cTKT9Vd9H6OWHgHA2iuvwKw5c3lz9pw++a5aOJWHdUSsHhHbldPDImJE1TXVv2LQII66+2pOev5Oplx3M4/ffs/8ZRvvviPt19/CG6++NoAtlOrvqZemMfXJ59hwjVUWmL/uKity/T1TAbjv8ad45uV/8Nwrr/LoMy9y7Z0Pcu7hX+Dio8YzaFBw1e33N1XruWmvsuIyIwEY3DaI4cOGMG3G6wusc93dU1lv9IosvpgnYOug0v8LETEemAAsC6wFjAZOBcZVWVf9K+fN4/hNdmbYUiM54LLTWPn96/L0Aw8B8KHPfpJbzrxggFso1dvMN97ksNMu4d/33J7hw4YssGy/Hbfi+xdPYs/jz2CdlUex3qrvo61tELe1P8aUvz3LPif+EoA33pzDsiOWBODQUy/mqZemMXvOPJ555R/sefwZAHxum83ZbauNgKSr6DT916df4MeX/ZHTDt6nku+rdy46n7Ls841H3ANsDtyWmZuU8+7LzA17WH8CRbgDnJ6Zp1fWOFXl28CMiJiemZcADwGrAG8MbLOkemo0GosBVwIT29vbf/g26wbwGPBB4AvAyu3t7Ud0XiciJnT87mw0GmsA57S3t2/dZTsTgWPb29tvbTQag4FngRXa29uz0WiMBv4IfKm9vf2WvviOWnhVnwaflZlvdnyIiMF09yddKTNPz8wx5cugXjSsACxdTg8DtgOmUvzRtSfFLyGDWupGGb5nAVN6CupGo7F0o9FYvPy4P3BTe3v7dOB64NONRmNUud6yjUZjdd464OnNFRRhD/Bp4I9lUC8NXAUcYVDXS9UXI26MiCOBYRGxPXAQ8PuKa6p/rQScC7RR/PF3EUVAHwvsDZw4YC2T6u8jwL7AfY1Go+NmjyOB1QDa29tPBdYHzms0GnOBB4H9ymUPNhqNo4FJjUZjEDAb+N9N1j0LOL/RaPwVeJliXwX4KrA2cEyj0TimnLdDe3v78wvxHdUHqj4NPojiH9YOFJdEJgJnZpVFVQsR8d+ZOWag2yG917jvtaaqj6x3Bc7LzDMqrqP68TKGNDDc91pQ1UfWZwPbAjcBFwATM9OH9iRJegcqDWuAiFgM+BjwGWAscF1m7l9pUUmSWkjlnaJk5mzgGooj6zspTo2rhiJibkTcExH3R8TFEbHEu9jGmRGxQTl9ZJdlf+6rtkqtICIyIk7u9PmbEXFsBXXcFxdxVZ8G34niLsNtgBuAC4FJngqvp4h4LTOHl9O/Bu7MzF6f+2x2e5L+WUS8ATwDfCgzX4yIbwLDM/PYPq7jvriIq/rI+ovA74B1M/MLmXm1Qb3I+BPFIxxExGHl0fb9EXFoOW/JiLgqIiaX8z9Tzr8hIsZExIkUj+zdUwY/EfFa+X5hROzcUSgizomIPSKiLSJOiog7IuLeiPhKf39pqZ/Nobgh7OtdF0TEChFxSbk/3BERH+k0/7qIuCsiTouIJyJi+XLZ7yLizoh4oOxkCvfFFpGZvnyRmQCvle+DgcuBA4HNgPuAJYHhwAPAJsAewBmdfnap8v0GYEzn7XWz/d2Bc8vpxYEnKTpUmQAcXc4fAvw3sOZA/3fx5auqF/AaMBJ4HFgK+CZwbLnsP4Gx5fRqwJRy+qfAEeX0ThQdTS1ffl62fB8G3A8s11Gna93y3X1xEXlV8uhWRNycmWMj4lUW7LEsgMzMkVXU1UIbVnYRC8WR9VkUgX1ZZs4AiIhLgY8C1wI/iIjvA1dm5p/eQZ1rgJ9ExBCKXzY3ZebrEbED8MGI+HS53lLAOhTdK0otKTOnR8R5wMFA59E0tgM2iJjfa/fIciCksRQhS2ZeGxGvdPqZgyNi93J6VYr956VeyrsvLiIqCevMHFu+O8LWouX1zNy484zo9Juis8x8KCI2A3YGToiISZl5XDNFMvONiLgB2JHiKYHfdJQDvpaZE9/tF5AWUT8G7gLO7jRvELBlZi4wHFZP+2REbE0R8Ftm5sxyHxvaW1H3xUVHpdesI+L8Zuap1m4CdouIJSJiSYq/6P8UESsDMzPzV8APgE27+dnZ5aN73bkA+BLFUXrHL4SJwIEdPxMR65Y1pZaWmS9TdNW7X6fZkyi6/wQgIjr+kL4Z2KuctwOwTDl/KeCVMqjXAz7caVvui4u4qm8we3/nD+VAHptVXFN9KDPvAs4Bbgduo+gu9m5gQ+D28rT5UcD3uvnx04F7O25q6WIS8K/AH/KtwV7OpOj7+K6IuB84jep72ZPq4mRg+U6fDwbGlDd4PQgcUM7/DrBDRNxF0YfFM8CrFJemBkfEvcB3gb902pb74iKukke3IuIIis7ohwEzO2YDb1IMfXlETz8rSepZeX15bmbOiYgtgV90vXyl1lP1c9YnGMyS1HciYh2KU+aDKA6ADsrMOwa2Vapaf3Q3ugzFXYTzb3TIzJsqLSpJUgup9BpEROwPHAKMBu6huOHhVorBPSRJUhOqvsHsEOBDwBOZuQ1FZxovVFxTkqSWUnVYv5GZb0BxU0RmTgUaFdeUJKmlVB3Wf4+IpSn6B78uIi4Hnq64prRIiT4Y7azTtraOiCvL6U9GxLd6WXfpiDjoXdQ4NooBJ5qa32Wdczr1itVMrTXKR4ek97RKwzozd8/MaVmMIHMMRfeVu1VZU1oEvZ6ZG2fmByju7j2g88IovON9NTOvyMwTe1llaeAdh7Wk/ld1D2bLdrwoBoO4mQX7Cpe0oD8Ba5dHlFMi4ucU3VCuGhE7RMSt5WhLF0dEx3CmO0XE1Ii4GfhUx4Yi4osR8dNyesWIuCyKUdImR8RWwInAWuVR/Unleod3GmnpO522dVREtEfEH2jiUlZEjC+3MzmKkaM6ny3YLiL+FBEPRcQnyvUd5UnqRdWnwe+iuKHsIeDhcvqx8peNPZlJnZQ9/H2M4g9bKELxvMzcBJgBHA1sl5mbUoyCdFhEDAXOAHah6C7yfT1s/ifAjZm5EUXXsA8A3wIeKY/qDy+7rlwH2BzYGNgsIv613Ff3prhB9FMUN42+nUsz80NlvSks2I3mGsC/AR8HTi2/w37APzLzQ+X2x0fEmk3Ukd4Tqu4+7lqKEZsmwvx+bHeieKD/58AWFdeXFgXdjXa2MsVTFB1dRn4Y2AC4pRzHYXGKxyDXAx7LzIcBIuJXFMMbdrUt8L8AMnMu8I+yD4TOdihfd5efh1OE9wiK/XhmWeOKJr7TByLiexSn2ofzVp/TABdl5jzg4Yh4tPwOPY3y9FATtaSWV3VYj8nM+dffMnNSRPzfzDys7DJPUvejnUFxND1/FnBdZn62y3ob03eXlgI4ITNP61Lj0HdR4xxgt8ycHBFfBLbutKzrtpIeRnmKiDXeYV2pJVV9GvzliPg/EbF6+fp34JWIaAPmVVxbaiV/AT4SEWsDRDEK2rrAVGDNiFirXO+zPfz89RRjk3dcHx5JMfhD52FsJwJf7nQtfJWIGEUx8truETEsivGUd2mivSOAZ6IYtelzXZbtGRGDyjb/C9COozxJvar6yHof4NsUj25BcYPZPkAb5RBvkt5eZr5QHqH+ptNZqaPLccUnAFdFxIsU+9gHutnEIcDpEbEfMBc4MDNvjYhbykejrimvW68P3Foe2b8GfD4z74qICyl6IXyC4lT92zmGYpS2JyiuwXf+o6AduBFYETigHFP5TIpr2XdFUfwFfHJEmq/yvsEBImJ4Zr5WeSFJklpQ1Y9ubRXFOKwPlp83Kh9FkSRJTar6mvWPgB2BlwAyczLFIOeSJKlJVYc1mflkl1lzq64pSVIrqfoGsyfLnpIyIhYHDqboIEGSJDWp0hvMImJ54P8D21E8RzkJOCQzX6qsqCRJLaZf7gaXJEnvXiWnwSPiP3pZnJn53SrqSpLUiio5so6Ib3Qze0mKzvqXy8zhfV5UkqQWVflp8LJ7wkMogvoi4OTMfL7SopIktZDK7gYvx7A+jKJf4HOBTTPzlarqSZLUqqq6Zn0Sxbi3pwMb2tWoJEnvXlXXrOcBs4A5LDgcXlDcYDayz4tKktSifHRLkqSaq7y7UUmStHAMa0mSas6wliSp5gxrSZJqzrCWJKnm/gfNtR2YGo+M6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "sns.heatmap(cm, square = True, annot = True, cmap = 'RdBu', cbar = False, \n",
    "            xticklabels = ['Positive', 'Negative'], yticklabels = ['Positive', 'Negative'])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    859\n",
       "1    329\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[771,  88],\n",
       "       [ 37, 292]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check which are wrong labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_predictions = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if predictions[i] == 1:\n",
    "        testing_predictions.append(1)\n",
    "    else:\n",
    "        testing_predictions.append(0)\n",
    "\n",
    "check_df = pd.DataFrame({'actual_label': list(y_test), 'prediction': testing_predictions, 'abstract':list(x_test)})\n",
    "check_df.replace(to_replace=0, value= 'Positive', inplace=True)\n",
    "check_df.replace(to_replace=1, value= 'Negative', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I love simsimi! #simsimi #me #iphone #app #awe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@SamsungMobile #yousuck Worst mobiles with wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>#Beauty #Shopping #AliUSAExpress #iPhone #ios ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Well, well, what do we have here? #Google #And...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>running 10.5.7 and everything looks fine! my i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Got some much needed time w baby Nolan and mam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>...and spell check if I mistyped \"quickest\" ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Only reason I'd get an iPhone is b/c of all th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>#happiness is #new #gadget #gift fromm #brothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Hi, Robot,Thx Follow Us #InstantUnlock #APPLE ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1188 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual_label prediction  \\\n",
       "0        Positive   Positive   \n",
       "1        Negative   Negative   \n",
       "2        Positive   Positive   \n",
       "3        Positive   Negative   \n",
       "4        Positive   Negative   \n",
       "...           ...        ...   \n",
       "1183     Positive   Positive   \n",
       "1184     Negative   Negative   \n",
       "1185     Positive   Negative   \n",
       "1186     Positive   Positive   \n",
       "1187     Positive   Positive   \n",
       "\n",
       "                                               abstract  \n",
       "0     I love simsimi! #simsimi #me #iphone #app #awe...  \n",
       "1     @SamsungMobile #yousuck Worst mobiles with wor...  \n",
       "2     #Beauty #Shopping #AliUSAExpress #iPhone #ios ...  \n",
       "3     Well, well, what do we have here? #Google #And...  \n",
       "4     running 10.5.7 and everything looks fine! my i...  \n",
       "...                                                 ...  \n",
       "1183  Got some much needed time w baby Nolan and mam...  \n",
       "1184  ...and spell check if I mistyped \"quickest\" ho...  \n",
       "1185  Only reason I'd get an iPhone is b/c of all th...  \n",
       "1186  #happiness is #new #gadget #gift fromm #brothe...  \n",
       "1187  Hi, Robot,Thx Follow Us #InstantUnlock #APPLE ...  \n",
       "\n",
       "[1188 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   4,    9,   15,   24,   27,   36,   69,   70,   74,   78,   89,\n",
       "          93,  102,  107,  109,  131,  140,  145,  161,  165,  172,  182,\n",
       "         185,  192,  195,  203,  208,  216,  221,  227,  237,  238,  240,\n",
       "         242,  253,  280,  282,  292,  300,  306,  312,  315,  333,  343,\n",
       "         344,  348,  353,  360,  368,  377,  381,  384,  389,  392,  401,\n",
       "         408,  420,  423,  424,  454,  455,  468,  482,  494,  512,  515,\n",
       "         528,  559,  560,  568,  581,  590,  594,  596,  597,  606,  607,\n",
       "         614,  618,  629,  632,  640,  644,  670,  674,  685,  722,  723,\n",
       "         735,  746,  748,  764,  775,  781,  784,  800,  802,  817,  827,\n",
       "         832,  849,  853,  857,  860,  861,  872,  874,  879,  888,  891,\n",
       "         893,  905,  922,  931,  955,  965,  976, 1018, 1020, 1028, 1031,\n",
       "        1038, 1043, 1062, 1065, 1077, 1079, 1089, 1103, 1122, 1130, 1151],\n",
       "       dtype=int64),)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(check_df.actual_label != check_df.prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/zenit/Desktop/Sahal backup/Northeastern University/Datasets/AV identify sentiments/test.csv\")\n",
    "test1 = test['tweet']\n",
    "cv_test = cv.transform(test1)\n",
    "y_pred_nb = naive_bayes.predict(cv_test)\n",
    "pred = pd.DataFrame(y_pred_nb)\n",
    "pred.to_csv(\"C:/Users/zenit/Desktop/Sahal backup/Northeastern University/Datasets/AV identify sentiments/predict2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = [\"the house had a tiny little mouse\",\n",
    "      \"the cat saw the mouse\",\n",
    "      \"the mouse ran away from the house\",\n",
    "      \"the cat finally ate the mouse\",\n",
    "      \"the end of the mouse story\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = HashingVectorizer(stop_words='english', alternate_sign = False, n_features=opts.n_features)\n",
    "# doc_train_hash = vectorizer.transform(doc)\n",
    "\n",
    "vectorizer=TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "doc_train_vect = vectorizer.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 65536)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_train_hash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "get_feature_names_ not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-297-70d077d77b49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc_train_hash\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.julia\\conda\\3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: get_feature_names_ not found"
     ]
    }
   ],
   "source": [
    "doc_train_hash.get_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x11 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_train_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "word_count_vectorizer = cv.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "\n",
    "counting the occurrences of tokens in each document.\n",
    "\n",
    "normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "In this scheme, features and samples are defined as follows:\n",
    "\n",
    "each individual token occurrence frequency (normalized or not) is treated as a feature.\n",
    "\n",
    "the vector of all the token frequencies for a given document is considered a multivariate sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x16 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vectorizer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 2, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 0]], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vectorizer.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = cv.build_analyzer()\n",
    "analyze(\"the dog the sentence\") == ([\"the\", \"dog\", \"the\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ate',\n",
       " 'away',\n",
       " 'cat',\n",
       " 'end',\n",
       " 'finally',\n",
       " 'from',\n",
       " 'had',\n",
       " 'house',\n",
       " 'little',\n",
       " 'mouse',\n",
       " 'of',\n",
       " 'ran',\n",
       " 'saw',\n",
       " 'story',\n",
       " 'the',\n",
       " 'tiny']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The converse mapping from feature name to column index is stored in the vocabulary_ attribute of the vectorizer:\n",
    "\n",
    "cv.vocabulary_.get('mouse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform([\"mouse\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range = (1,2), token_pattern = r'\\b\\w+\\b', min_df = 1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze(\"Bi-grams are cool\") == ([\"b\", \"grams\", \"are\", \"cool\", \"bi-grams\", \"grams are\", \"are cool\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2 = bigram_vectorizer.fit_transform(doc).toarray()\n",
    "\n",
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a tiny',\n",
       " 'ate',\n",
       " 'ate the',\n",
       " 'away',\n",
       " 'away from',\n",
       " 'cat',\n",
       " 'cat finally',\n",
       " 'cat saw',\n",
       " 'end',\n",
       " 'end of',\n",
       " 'finally',\n",
       " 'finally ate',\n",
       " 'from',\n",
       " 'from the',\n",
       " 'had',\n",
       " 'had a',\n",
       " 'house',\n",
       " 'house had',\n",
       " 'little',\n",
       " 'little mouse',\n",
       " 'mouse',\n",
       " 'mouse ran',\n",
       " 'mouse story',\n",
       " 'of',\n",
       " 'of the',\n",
       " 'ran',\n",
       " 'ran away',\n",
       " 'saw',\n",
       " 'saw the',\n",
       " 'story',\n",
       " 'the',\n",
       " 'the cat',\n",
       " 'the end',\n",
       " 'the house',\n",
       " 'the mouse',\n",
       " 'tiny',\n",
       " 'tiny little']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The lower the IDF value of a word, the less unique it is to any particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         idf_weights\n",
       "mouse       1.000000\n",
       "the         1.000000\n",
       "cat         1.693147\n",
       "house       1.693147\n",
       "ate         2.098612\n",
       "away        2.098612\n",
       "end         2.098612\n",
       "finally     2.098612\n",
       "from        2.098612\n",
       "had         2.098612\n",
       "little      2.098612\n",
       "of          2.098612\n",
       "ran         2.098612\n",
       "saw         2.098612\n",
       "story       2.098612\n",
       "tiny        2.098612"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(), columns=['idf_weights'])\n",
    "df_idf.sort_values(by=[\"idf_weights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x16 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector = cv.transform(doc)\n",
    "count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector = tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15)\t0.4935620852501244\n",
      "  (0, 14)\t0.23518497814732847\n",
      "  (0, 9)\t0.23518497814732847\n",
      "  (0, 8)\t0.4935620852501244\n",
      "  (0, 7)\t0.39820278266020154\n",
      "  (0, 6)\t0.4935620852501244\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf\n",
       "had      0.493562\n",
       "little   0.493562\n",
       "tiny     0.493562\n",
       "house    0.398203\n",
       "mouse    0.235185\n",
       "the      0.235185\n",
       "ate      0.000000\n",
       "away     0.000000\n",
       "cat      0.000000\n",
       "end      0.000000\n",
       "finally  0.000000\n",
       "from     0.000000\n",
       "of       0.000000\n",
       "ran      0.000000\n",
       "saw      0.000000\n",
       "story    0.000000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "first_document_vector = tf_idf_vector[0]\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index = feature_names, columns = [\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.7, smooth_idf = True)#, #ngram_range = (1,4), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09861229, 2.09861229, 1.69314718, 2.09861229, 2.09861229,\n",
       "       1.69314718, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.09861229])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tfidf_vectorizer.fit_transform(doc)\n",
    "tfidf_vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.49552379, 0.61418897, 0.        , 0.        , 0.        ,\n",
       "        0.61418897],\n",
       "       [0.        , 0.        , 0.62791376, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.77828292, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.61418897, 0.        , 0.        , 0.        ,\n",
       "        0.49552379, 0.        , 0.61418897, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.61418897, 0.        , 0.49552379, 0.        , 0.61418897,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.70710678, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.70710678,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ate',\n",
       " 'away',\n",
       " 'cat',\n",
       " 'end',\n",
       " 'finally',\n",
       " 'house',\n",
       " 'little',\n",
       " 'ran',\n",
       " 'saw',\n",
       " 'story',\n",
       " 'tiny']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.7, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5544,)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5544x172956 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 276857 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5544, 65749)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 55293)\t0.21491437038138658\n",
      "  (0, 46757)\t0.20515349305213257\n",
      "  (0, 28256)\t0.20515349305213257\n",
      "  (0, 10913)\t0.16985394207816215\n",
      "  (0, 56007)\t0.1982280403735304\n",
      "  (0, 5145)\t0.21491437038138658\n",
      "  (0, 31542)\t0.12611092191201465\n",
      "  (0, 32448)\t0.20515349305213257\n",
      "  (0, 63173)\t0.21491437038138658\n",
      "  (0, 11277)\t0.21491437038138658\n",
      "  (0, 11223)\t0.21491437038138658\n",
      "  (0, 41903)\t0.21491437038138658\n",
      "  (0, 35554)\t0.21491437038138658\n",
      "  (0, 50327)\t0.21491437038138658\n",
      "  (0, 21361)\t0.21491437038138658\n",
      "  (0, 2401)\t0.21491437038138658\n",
      "  (0, 55292)\t0.1982280403735304\n",
      "  (0, 46755)\t0.1928562419434394\n",
      "  (0, 28158)\t0.0446579384186658\n",
      "  (0, 10863)\t0.1092746021008594\n",
      "  (0, 56003)\t0.12932546125695063\n",
      "  (0, 4386)\t0.05091884164387218\n",
      "  (0, 31494)\t0.04407297014700945\n",
      "  (0, 32435)\t0.14340673474105192\n",
      "  (0, 63137)\t0.12354447531723062\n",
      "  :\t:\n",
      "  (5543, 53088)\t0.2375408217932295\n",
      "  (5543, 18185)\t0.2375408217932295\n",
      "  (5543, 3074)\t0.2375408217932295\n",
      "  (5543, 53476)\t0.2375408217932295\n",
      "  (5543, 48118)\t0.2375408217932295\n",
      "  (5543, 16285)\t0.2375408217932295\n",
      "  (5543, 30574)\t0.2375408217932295\n",
      "  (5543, 45022)\t0.2375408217932295\n",
      "  (5543, 58730)\t0.2375408217932295\n",
      "  (5543, 45021)\t0.2375408217932295\n",
      "  (5543, 53475)\t0.20830921967260557\n",
      "  (5543, 48105)\t0.1822115592258371\n",
      "  (5543, 21966)\t0.20065464674830122\n",
      "  (5543, 53082)\t0.1975207050744458\n",
      "  (5543, 57421)\t0.1857645292591006\n",
      "  (5543, 3068)\t0.18773637572241494\n",
      "  (5543, 31860)\t0.15717410878836915\n",
      "  (5543, 18155)\t0.15850477360179105\n",
      "  (5543, 16178)\t0.11175566427053757\n",
      "  (5543, 28229)\t0.0878983648900766\n",
      "  (5543, 30000)\t0.08785037975370363\n",
      "  (5543, 21931)\t0.10466628561802187\n",
      "  (5543, 30556)\t0.06118354785230065\n",
      "  (5543, 28158)\t0.04935958155211411\n",
      "  (5543, 31494)\t0.0487130270954455\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2376,)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2376x65749 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 38619 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac = PassiveAggressiveClassifier(max_iter = 200)\n",
    "pac.fit(tfidf_train, y_train)\n",
    "\n",
    "y_pred = pac.predict(tfidf_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[794,  65],\n",
       "       [ 67, 262]], dtype=int64)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/zenit/Desktop/Sahal backup/Northeastern University/Datasets/AV identify sentiments/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7921</td>\n",
       "      <td>I hate the new #iphone upgrade. Won't let me d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7922</td>\n",
       "      <td>currently shitting my fucking pants. #apple #i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7923</td>\n",
       "      <td>I'd like to puts some CD-ROMS on my iPad, is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7924</td>\n",
       "      <td>My ipod is officially dead. I lost all my pict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7925</td>\n",
       "      <td>Been fighting iTunes all night! I only want th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>9869</td>\n",
       "      <td>#SamsungGalaxyNote7 Explodes, Burns 6-Year-Old...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>9870</td>\n",
       "      <td>Now Available - Hoodie. Check it out here - ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>9871</td>\n",
       "      <td>There goes a crack right across the screen. If...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>9872</td>\n",
       "      <td>@codeofinterest as i said #Adobe big time we m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>9873</td>\n",
       "      <td>Finally I got it .. thanx my father .. #Samsun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1953 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet\n",
       "0     7921  I hate the new #iphone upgrade. Won't let me d...\n",
       "1     7922  currently shitting my fucking pants. #apple #i...\n",
       "2     7923  I'd like to puts some CD-ROMS on my iPad, is t...\n",
       "3     7924  My ipod is officially dead. I lost all my pict...\n",
       "4     7925  Been fighting iTunes all night! I only want th...\n",
       "...    ...                                                ...\n",
       "1948  9869  #SamsungGalaxyNote7 Explodes, Burns 6-Year-Old...\n",
       "1949  9870  Now Available - Hoodie. Check it out here - ht...\n",
       "1950  9871  There goes a crack right across the screen. If...\n",
       "1951  9872  @codeofinterest as i said #Adobe big time we m...\n",
       "1952  9873  Finally I got it .. thanx my father .. #Samsun...\n",
       "\n",
       "[1953 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_test = cv.transform(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-8ec7b444f96b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_test1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_test1 = tfidf_vectorizer.transform(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1953x206161 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 40301 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = pac.predict(tfidf_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nb = naive_bayes.predict(cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1953 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0     1\n",
       "1     0\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "...  ..\n",
       "1948  0\n",
       "1949  0\n",
       "1950  1\n",
       "1951  1\n",
       "1952  0\n",
       "\n",
       "[1953 rows x 1 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_csv(\"C:/Users/zenit/Desktop/Sahal backup/Northeastern University/Datasets/AV identify sentiments/predict2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
